<!doctype html>
<html>

<head>
	<meta charset="utf-8">
	<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

	<title>Sequential Monte Carlo in Approximate Bayesian Computation</title>

	<link rel="stylesheet" href="css/reveal.css">
	<link rel="stylesheet" href="css/theme/white.css">

	<!-- Theme used for syntax highlighting of code -->
	<link rel="stylesheet" href="lib/css/zenburn.css">

	<script src="js/jquery-1.4.4.min.js"></script>

	<!-- Printing and PDF exports -->
	<script>
		var link = document.createElement('link');
		link.rel = 'stylesheet';
		link.type = 'text/css';
		link.href = window.location.search.match(/print-pdf/gi) ? 'css/print/pdf.css' : 'css/print/paper.css';
		document.getElementsByTagName('head')[0].appendChild(link);
	</script>

	<script type="text/x-mathjax-config">
		MathJax.Hub.Config({ TeX: { extensions: ["color.js"] }});
	</script>

</head>

<body>
	<div class="reveal">
		<div class="slides">
			<section>
				<h2>Sequential Monte Carlo in Approximate Bayesian Computation</h2>
			</section>

			<section>
				<h2>Overview</h2>
				<ul>
					<li>Motivation</li>
					<li>Approximate Bayesian Computation</li>
					<li>SMC in ABC
						<ul>
							<li>Focus on algorithm developed by Del Moral et. al. (2012)</li>
						</ul>
					</li>
					<li>Examples</li>
				</ul>
			</section>

			<section>
				<h2>Bayesian Inference</h2>

				<div style="position:relative; width:640px; height:480px; margin:0 auto;">

					<div class="fragment" width="640" height="480" style="position:absolute;top:0;left:0;" data-fragment-index="0">
						<ul>
							<li>$p(\boldsymbol{\theta}|\boldsymbol{y})\propto p(\boldsymbol{y}|\boldsymbol{\theta})p(\boldsymbol{\theta})$</li>
							<li>$\boldsymbol{\theta}$ are parameters of some model</li>
							<li>$\boldsymbol{y}$ is observed data
						</ul>
						<div class="fragment fade-in" width="640" height="480" style="position:absolute;top:0;left:0;" data-fragment-index="1">
							<div class="fragment fade-out" width="640" height="480" style="position:absolute;top:0;left:0;" data-fragment-index="2">
								<ul>
									<li>$p(\boldsymbol{\theta}|\boldsymbol{y})\propto \color{red} p(\boldsymbol{y}|\boldsymbol{\theta})\color{black}p(\boldsymbol{\theta})$</li>
									<li></li>
									<li></li>
									<li>
										<span style="color:red">Likelihood</span>
									</li>
								</ul>
							</div>
						</div>

						<div class="fragment fade-in" width="640" height="480" style="position:absolute;top:0;left:0;" data-fragment-index="2">
							<ul>
								<li>$p(\boldsymbol{\theta}|\boldsymbol{y})\propto \color{red} p(\boldsymbol{y}|\boldsymbol{\theta})\color{blue}p(\boldsymbol{\theta})$</li>
								<li></li>
								<li></li>
								<li>
									<span style="color:red">Likelihood</span>: Model of data
								</li>
								<li>
									<span style="color:blue">Prior</span>: Prior knowledge</li>
							</ul>
						</div>
					</div>
			</section>

			<section>
				<h2>Posterior</h2>
				<ul>
					<li class="fragment">Potentially difficult to evaluate</li>
					<li class="fragment">Many methods exist for approximating a distribution</li>
				</ul>
			</section>

			<section>
				<h2>Approximating the posterior</h2>
				<ul>
					<li class="fragment">MCMC methods</li>
					<ul>
						<li class="fragment">Popular for sampling from arbitrary distribution</li>
					</ul>
					<li class="fragment">Importance sampling</li>
					<li class="fragment">Need to evaluate likelihood</li>
				</ul>
			</section>

			<section>
				<h2>Approximate Bayesian Computation</h2>
				<ul>
					<li class="fragment">Approximation to distribution without evaluating likelihood</li>
					<li class="fragment">Also known as Likelihood-free methods</li>
					<li class="fragment">First used in population genetics</li>
					<li class="fragment">Can be used where model is some stochastic process</li>
					<li class="fragment">Need to be able to generate (many) samples from likelihood</li>
				</ul>
			</section>

			<section>
				<h2>Illustration of core idea</h2>
				<ul>
					<li class="fragment>">Introduce an auxillary variable $\boldsymbol{x}$</br>
						$p(\boldsymbol{\theta},\boldsymbol{x}|\boldsymbol{y})\propto p(\boldsymbol{y}|\boldsymbol{x},\boldsymbol{\theta})p(\boldsymbol{x}|\boldsymbol{\theta})p(\boldsymbol{\theta})$</li>
					<li class="fragment" data-fragment-index="1">Assuming $p(\boldsymbol{y}|\boldsymbol{x},\boldsymbol{\theta})$ has point mass $\boldsymbol{x}=\boldsymbol{y}$
						<br/> Posterior can be retrieved by integrating out $\boldsymbol{x}$
						<br/> $\begin{align*}p(\boldsymbol{\theta}|\boldsymbol{y}) & \propto\int_{Y}p(\boldsymbol{y}|\boldsymbol{x},\boldsymbol{\theta})p(\boldsymbol{x}|\boldsymbol{\theta})d\boldsymbol{x}p(\boldsymbol{\theta})\\ & =p(\boldsymbol{y}|\boldsymbol{\theta})p(\boldsymbol{\theta})\end{align*}$
					</li>
				</ul>
			</section>

			<section>
				<h2>Relaxation of requirements</h2>
				<ul>
					<li>Generate samples from the likelihood that lie close to the observed data</li>
					<li>$\boldsymbol{x}\approx\boldsymbol{y}$</li>
					<li>Let $p(\boldsymbol{y}|\boldsymbol{x},\boldsymbol{\theta})$ be some kernel density function centered on $\boldsymbol{y}$</li>
					<li>$d(\boldsymbol{x},\boldsymbol{y})
						<\epsilon$</li>
				</ul>
			</section>

			<section>
				<ul>
					<li>Another relaxation of requirements</li>
					<li>Use statistics instead of data itself
						<br/> $d(S(\boldsymbol{x}),S(\boldsymbol{y}))
						<\epsilon$ </li>
							<li class="fragment">$S(\boldsymbol{x})\approx S(\boldsymbol{y})$ more likely than $\boldsymbol{x}\approx\boldsymbol{y}$</li>
							<li class="fragment">Would like the statistics to be sufficient for the parameters being estimated
								<br/> $p(S(\boldsymbol{y})|\boldsymbol{\theta})p(\boldsymbol{\theta}) = p(\boldsymbol{y}|\boldsymbol{\theta})p(\boldsymbol{\theta})$
							</li>
				</ul>
			</section>

			<section>
				<h2>Problems</h2>
				<ul>
					<li class="fragment">Finding statistics
						<ul>
							<li class="fragment">Ad-hoc</li>
							<li class="fragment">Not focus here</li>
						</ul>
					</li>

					<li class="fragment">Getting a good approximation to posterior
						<br> $\epsilon \to 0$
					</li>

					<li class="fragment">Computational resources
					</li>
				</ul>
			</section>

			<section>
				<ul>
					<li>Early example from Marjoram et al. (2003)</li>
				</ul>
				<img width="846" height="613" src="figures/mcmc_in_abc.png" />
			</section>

			<section>
				<h2>SMC in ABC</h2>
				<ul>
					<li class="fragment">Focus here is on an algorithm developed by Del Moral et. al. (2012)</li>
					<li class="fragment">Algorithm uses what is called an SMC sampler developed by Del Moral et. al. (2006)</li>
				</ul>
			</section>

			<section>
				<h2>SMC sampler</h2>
				<ul>
					<li>Detour into Sequential Monte Carlo</li>
				</ul>
			</section>

			<section>
				<h2>Importance sampling</h2>
				<ul>
					<li>Target distribution, $p(\boldsymbol{x})$
						<li class="fragment">Proposal distribution, $g(\boldsymbol{x})$
							<br/>
							<img width="362" height="358" src="figures/targetAndProposalIllustration.png" />
						</li>
						<li class="fragment">Generate weighted sample: $w(\boldsymbol{x})=\frac{p(\boldsymbol{x})}{g(\boldsymbol{x})}$</li>
				</ul>
			</section>

			<section>
				<h2>Sequential Importance Resampling</h2>
				<ol>
					<li>Sample $\boldsymbol{x}_{1},...,\boldsymbol{x}_{n}$ using probability distribution for $g(\boldsymbol{x})$</li>
					<li>Calculate the weights, $w(\boldsymbol{x}_{i})$</li>
					<li>Resample from the set $\boldsymbol{x}_{1},...,\boldsymbol{x}_{n}$ $m$ times with probabilities for drawing $\boldsymbol{x}_{i}$ being proportional to the weight $w(\boldsymbol{x}_{i})$. </li>
				</ol>
			</section>

			<section>
				<h2>Normalised weights</h2>
				<ul>
					<li>If the target distribution is only known up to a constant: $\mathrm{c}p(\boldsymbol{x})$</li>
					<li class="fragment" data-fragment-index="1">$w(\boldsymbol{x}_{i})=\frac{p(\boldsymbol{x}_{i})/g(\boldsymbol{x}_{i})}{\sum_{j=1}^{n}p(\boldsymbol{x}_{j})/g(\boldsymbol{x}_{j})}$</li>
					<li class="fragment" data-fragment-index="1">The constant cancels</li>
				</ul>
			</section>

			<section>
				<h2>Sequential Monte Carlo</h2>
				<ul>
					<li class="fragment" data-fragment-index="1">Idea is to split problem of computing $w(\boldsymbol{x})=\frac{p(\boldsymbol{x})}{g(\boldsymbol{x})}$ into substeps</li>
					<li class="fragment" data-fragment-index="2">$w(\boldsymbol{x})=\frac{p(x_{1})p(x_{2}|x_{1})p(x_{3}|x_{1},x_{2})...p(x_{k}|x_{1},...,x_{k-1})}{g(x_{1})g(x_{2}|x_{1})g(x_{3}|x_{1},x_{2})...g(x_{k}|x_{1},...,x_{k-1})}$</li>
					<li class="fragment" data-fragment-index="3">Define $\boldsymbol{x}_{t}=(x_{1},...,x_{t})$
						<br/> $w(\boldsymbol{x}_{t})=w(\boldsymbol{x}_{t-1})\frac{p(x_{t}|\boldsymbol{x}_{t-1})}{g(x_{t}|\boldsymbol{x}_{t-1})}$
					</li>
					<li class="fragment" data-fragment-index="4">High-dimensional problems</li>
					<li class="fragment" data-fragment-index="4">Can make use of $p(x_{t}|\boldsymbol{x}_{t-1})$ when setting up the proposal distribution $g(x_{t}|\boldsymbol{x}_{t-1})$</li>
				</ul>
			</section>

			<section>
				<h2>A further approximation</h2>
				<ul>
					<li>Difficult to get $p(x_{k}|\boldsymbol{x}_{k-1})$</li>
					<li class="fragment">Introduce a sequence of approximations to marginal: $\hat{p}(\boldsymbol{x}_{k})$</li>
					<li class="fragment">Final step in approximation: $\hat{p}(\boldsymbol{x}_{k})=p(\boldsymbol{x}_{k})$</li>
					<li class="fragment">Weight update becomes:
						$$\begin{align*} w(\boldsymbol{x}_{t}) & =w(\boldsymbol{x}_{t-1})\frac{\hat{p}(\boldsymbol{x}_{t})}{\hat{p}(\boldsymbol{x}_{t-1})g(x_{t}|\boldsymbol{x}_{t-1})}\\ & =w(\boldsymbol{x}_{t-1})u_{t}(\boldsymbol{x}_{t-1}) \end{align*}$$
					</li>
					<li class="fragment">Approximations will cancel and final weight is: $w(\boldsymbol{x}_{k})=\frac{p(\boldsymbol{x}_{k})}{g(\boldsymbol{x}_{k})}$</li>
				</ul>
			</section>

			<section>
				<section>
					<h2>Weight degeneracy</h2>
					<ul>
						<li class="fragment">Weights will spread out</li>
						<li class="fragment">Small number of particles can dominate</li>
					</ul>
				</section>
				<section>
					<video width="600" height="600" controls>
						<source src="video/weight_plot_resample0.mp4" type="video/mp4"> Your browser does not support the video tag.
					</video>
				</section>
			</section>

			<section>
				<section>
					<h2>Possible remedy</h2>
					<ul>
						<li class="fragment">Introduce resampling steps</li>
						<li class="fragment">Resample with probability proportional to the weights</li>
						<li class="fragment">Triggered by a measure called Effective Sample Size (ESS)</li>
						<li class="fragment">How many samples are weighted ones compared to ones coming from the true distribution</li>
					</ul>
				</section>

				<section>
					<h2>ESS</h2>
					<ul>
						<li class="fragment">Definition used here:$$\frac{1}{\sum_{i=1}^{n}w(\boldsymbol{x}_{i})^{2}}$$</li>
						<li class="fragment">Valid when using normalised weights</li>
						<li class="fragment">Liu (2001):$$\mathrm{ESS}=\frac{n}{1+\mathrm{var}_{g}(w(\boldsymbol{X}))}$$</li>
					</ul>
				</section>

				<section>
					<section>
						<video width="600" height="600" controls>
							<source src="video/weight_plot_resample05.mp4" type="video/mp4"> Your browser does not support the video tag.
						</video>
					</section>
				</section>
			</section>

			<section>
				<h2>SMC sampler</h2>
				<ul>
					<li>Del Moral et. al. (2006)</li>
					<li class="fragment">The main idea is that the underlying probability space is the same through the weight updates</li>
					<li class="fragment">Achieved by using what is called backward kernels</li>
					<li class="fragment">$w(\boldsymbol{x}_{i})=\frac{p_{i}(\boldsymbol{x}_{i})\Pi_{t=2}^{i}L(\boldsymbol{x}_{t},\boldsymbol{x}_{t-1})}{g_{1}(\boldsymbol{x}_{1})\Pi_{t=1}^{i-1}K(\boldsymbol{x}_{t},\boldsymbol{x}_{t+1})}$</li>
				</ul>
			</section>

			<section>
				<h2>Optimal backward kernel</h2>
				<ul>
					<li>$L_{i-1}(\boldsymbol{x}_{i},\boldsymbol{x}_{i-1})=\frac{g_{i-1}(\boldsymbol{x}_{i-1})K_{i}(\boldsymbol{x}_{i-1},\boldsymbol{x}_{i})}{g_{i}(\boldsymbol{x}_{i})}$</li>
					<li>Optimal in sense of minimum variance of unormalised weights</li>
					<li class="fragment">Difficulty in computing marginals: $g(\boldsymbol{x}_{i})$</li>
					<li class="fragment">Instead approximate optimal backward kernel with an MCMC kernel with invariant distribution $p_{i}(\boldsymbol{z}_{i})$</li>
				</ul>
			</section>

			<section>
				<h2>Adaptive SMC method in ABC</h2>
				<ul>
					<li>Del Moral et. al. (2012)</li>
					<li>A particle: $\{\boldsymbol{\theta},\{\boldsymbol{x}\}_{1:m}\}$</li>
					<li>$p_{\epsilon}(\boldsymbol{\theta},\{\boldsymbol{x}\}_{1:m}|\boldsymbol{y})$</li>
					<li class="fragment">The marginal for $\boldsymbol{\theta}$ is the same for any $m$</li>
				</ul>
			</section>

			<section>
				<ul>
					<h2>Weight update</h2>
					<li>By using backward MCMC kernel</br>
						$\begin{align*}
w_{i}(\boldsymbol{z}_{i}) & =w_{i-1}(\boldsymbol{z}_{i-1})\frac{p_{i}(\boldsymbol{z}_{i-1})}{p_{i-1}(\boldsymbol{z}_{i-1})}\\
 & \propto w_{i-1}(\boldsymbol{z}_{i-1})\frac{\sum_{j=1}^{m}I_{\epsilon_{i},\boldsymbol{y}}(\boldsymbol{x}_{i-1}^{(j)})}{\sum_{j=1}^{m}I_{\epsilon_{i-1},\boldsymbol{y}}(\boldsymbol{x}_{i-1}^{(j)})}
\end{align*}$
					</li>
					<li class="fragment">Now need to specify kernel for propagating the particles</li>
				</ul>
			</section>

			<section>
				<ul>
					<li>Metropolis-Hastings ratio:
						<br/> $\begin{multline} \min\left(1,\frac{p_{\epsilon}(\boldsymbol{\theta}_{i+1},\{\boldsymbol{x}_{i+1}\}_{1:m}|\boldsymbol{y})q_{i}(\boldsymbol{\theta}_{i+1},\boldsymbol{\theta}_{i})\prod_{j=1}^{m}p(\boldsymbol{x}_{i}^{(j)}|\boldsymbol{\theta}_{i})}{p_{\epsilon}(\boldsymbol{\theta}_{i},\{\boldsymbol{x}_{i}\}_{1:m}|\boldsymbol{y})q_{i}(\boldsymbol{\theta}_{i},\boldsymbol{\theta}_{i+1})\prod_{j=1}^{m}p(\boldsymbol{x}_{i+1}^{(j)}|\boldsymbol{\theta}_{i+1})}\right)=\\
						\min\left(1,\frac{\sum_{j=1}^{m}\mathrm{I}_{\epsilon,\boldsymbol{y}}(\boldsymbol{x}_{i+1}^{(j)})q_{i}(\boldsymbol{\theta}_{i+1},\boldsymbol{\theta}_{i})p(\boldsymbol{\theta}_{i+1})}{\sum_{j=1}^{m}\mathrm{I}_{\epsilon,\boldsymbol{y}}(\boldsymbol{x}_{i}^{(j)})q_{i}(\boldsymbol{\theta}_{i},\boldsymbol{\theta}_{i+1})p(\boldsymbol{\theta}_{i})}\right)\label{eq:mh_ratio_without_likelihood}
						\end{multline}$
					</li>
				</ul>
			</section>

			<section>
				<section>
					<h2>Algorithm</h2>
				</section>

				<section>
					<!-- <img width="736" height="727" src="figures/smc_algorithm1.png"/> -->
					<img width="589" height="582" src="figures/smc_algorithm1.png" />
				</section>

				<section>
					<img src="figures/smc_algorithm2.png" />
				</section>

				<section>
					<img src="figures/smc_algorithm3.png" />
				</section>
			</section>

			<section>
				<h2>Resampling schedule</h2>
				<div class="fragment">$\mathrm{ESS}(w(\boldsymbol{x}_{i}))=\alpha\mathrm{ESS}(w(\boldsymbol{x}_{i-1}))$</div>
				<img src="figures/epsilon_candidate_plot_1000_09_toy_example.png" class="fragment" />
			</section>

			<section>
				<h1>Examples</h1>
			</section>

			<section>
				<section>
					<h2>Simple mixture</h2>
					<ul>
						<li>$p(\theta|x)\sim(\mathrm{N}(\theta,1)+\mathrm{N}(\theta,\frac{1}{100}))\mathrm{I}_{[-10,10]}(\theta)$</li>
						<li class="fragment">$d(x,y)=|x-y|$</li>
						<li class="fragment">Single observation at $y=0$</li>
					</ul>
				</section>

				<section>
					<h3>No replicates</h3>
					<video width="600" height="600" controls>
						<source src="video/configuration1_replicates.mp4" type="video/mp4">Your browser does not support the video tag.
					</video>
				</section>

				<section>
					<h3>5 replicates</h3>
					<video width="600" height="600" controls>
						<source src="video/configuration2_replicates.mp4" type="video/mp4">Your browser does not support the video tag.
					</video>
				</section>

				<section>
					<h3>20 replicates</h3>
					<video width="600" height="600" controls>
						<source src="video/configuration3_replicates.mp4" type="video/mp4">Your browser does not support the video tag.
					</video>
				</section>
			</section>

			<section>
				<section>
					<h2>MA(2) process</h2>
					<ul>
						<li>Example used in Marin et. al. (2012)</li>
						<li class="fragment">$z_{t}=a_{t}-\theta_{1}a_{t-1}-\theta_{2}a_{t-2}$</li>
						<li class="fragment">The task is to provide estimates for $\boldsymbol{\theta}$</li>
						<li class="fragment">Prior is chosen to be uniform on region that makes the process invertible</li>
						<li class="fragment">There is a closed expression for the likelihood</li>
					</ul>
				</section>

				<section>
					<h2>Setups</h2>
					<ul>
						<li>Trying two different statistics</li>
						<li>In all cases a time series of length 100 is generated, and this is used as the observation</li>
					</ul>
				</section>

				<section>
					<h3>Autocovariance distance</h3>
					<ul>
						<li>Using measure of autocovariance as statistic</li>
						<li class="fragment">$d=\frac{1}{n}\left(\sum_{i=2}^{n}\left(x_{i}x_{i-1}\right)-\sum_{i=2}^{n}\left(y_{i}y_{i-1}\right)\right)^{2}\\ +\frac{1}{n}\left(\sum_{i=2}^{n}\left(x_{i}x_{i-2}\right)-\sum_{i=2}^{n}\left(y_{i}y_{i-2}\right)\right)^{2}$
						</li>
					</ul>
				</section>

				<section>
					<h3>Autocovariance distance</h3>
					<video width="600" height="600" controls>
						<source src="video/ma_autocovariance.mp4" type="video/mp4">Your browser does not support the video tag.
					</video>
				</section>
			</section>

			<section>
				<h3>Raw distance function</h3>
				<ul>
					<li>$d=\sum_{i=1}^{n}\left(x_{i}-y_{i}\right)^{2}$</li>
				</ul>
			</section>

			<section>
				<section>
					<h3>Raw distance function</h3>
					<ul>
						<li>Same settings as in previous example</li>
					</ul>
					<video width="750" height="500" controls>
						<source src="video/ma_raw_distance.mp4" type="video/mp4">Your browser does not support the video tag.
					</video>
				</section>

				<section>
					<h3>What happens when it gets stuck?</h3>
					<div class="fragment">
						Note: Data from a different example
						<img src="figures/epsilon_candidate_plot.png" />
						<img src="figures/distance_to_epsilon.png" />
					</div>
				</section>

				<section>
					<h3>Raw distance function</h3>
					<ul>
						<li>$\alpha=0.99$</li>
					</ul>
					<br/>

					<video width="750" height="500" controls>
						<source src="video/ma_raw_distance2.mp4" type="video/mp4">Your browser does not support the video tag.
					</video>

				</section>

				<!--
					<section>
						<h3>Raw distance function</h3>
						<ul>
							<li>$\alpha=0.9$, 5 replicates</li>
							<li class="fragment" style="color:red">Bug</li>
						</ul>
						<br/>
						<video width="750" height="500" controls>
							<source src="video/ma_raw_distance3.mp4" type="video/mp4">Your browser does not support the video tag.
						</video>
					</section>
-->

				<section>
					<h3>Raw distance function</h3>
					<ul>
						<li>$\alpha=0.9$, 5 replicates</li>
						<li>Fixed</li>
					</ul>
					<br/>

					<video width="750" height="500" controls>
						<source src="video/ma_raw_distance3_v2.mp4" type="video/mp4">Your browser does not support the video tag.
					</video>
				</section>
			</section>

			<section>
				<ul>
					<li class="fragment">Method we present here requires many samples from approximation to likelihood</li>
				</ul>
			</section>

			</div>
		</div>

		<script src="lib/js/head.min.js"></script>
		<script src="js/reveal.js"></script>

		<script>
			// More info https://github.com/hakimel/reveal.js#configuration
			Reveal.initialize({
				history: true,

				math: {
					mathjax: 'https://cdn.mathjax.org/mathjax/latest/MathJax.js',
					// Switch to the following line if off-line and need to render math symbols
					//mathjax: 'http://localhost:8000/js/MathJax/MathJax.js',
					config: 'TeX-AMS_HTML-full' // See http://docs.mathjax.org/en/latest/config-files.html
				},


				// More info https://github.com/hakimel/reveal.js#dependencies
				dependencies: [{
						src: 'plugin/markdown/marked.js'
					}, {
						src: 'plugin/markdown/markdown.js'
					}, {
						src: 'plugin/notes/notes.js',
						async: true
					}, {
						src: 'plugin/highlight/highlight.js',
						async: true,
						callback: function() {
							hljs.initHighlightingOnLoad();
						}
					}, {
						src: 'plugin/math/math.js',
						async: true
					},
					// Zoom in and out with Alt+click
					{
						src: 'plugin/zoom-js/zoom.js',
						async: true
					}
				]
			});
		</script>
</body>

</html>
